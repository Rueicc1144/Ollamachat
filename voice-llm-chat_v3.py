# å¯ä»¥å³æ™‚æ‰“æ–·ï¼Œä½†å¥½å¥‡æ€ª
import torch
import numpy as np
import sounddevice as sd
import soundfile as sf
import threading
import queue
import time
import whisper
from opencc import OpenCC
import webrtcvad
import requests
import json
import io
from collections import deque
import os

class DialogController:
    def __init__(self):
        self.state = "idle"  # idle / listening / thinking / speaking
        self.audio_playing = False
        self.can_listen = True
        self.interrupted = False
        self.lock = threading.Lock()  # Add a lock for thread safety
    
    def start_listening(self):
        with self.lock:
            self.state = "listening"
            self.can_listen = True
    
    def start_thinking(self):
        with self.lock:
            self.state = "thinking"
            self.can_listen = False
    
    def start_speaking(self):
        with self.lock:
            self.state = "speaking"
            self.audio_playing = True
            self.can_listen = False  # Stop listening while speaking
            self.interrupted = False
    
    def stop_speaking(self):
        with self.lock:
            self.audio_playing = False
            self.state = "idle"
            self.can_listen = True
    
    def interrupt(self):
        with self.lock:
            if self.state == "speaking":
                self.interrupted = True
                return True
            return False
            
    def get_state(self):
        with self.lock:
            return self.state
            
    def can_be_interrupted(self):
        with self.lock:
            # Only allow interruption when speaking
            return self.state == "speaking"
            
    def should_listen(self):
        with self.lock:
            return self.can_listen

class AudioCapture:
    """
    éŸ³è¨Šæ“·å–æ¨¡çµ„ï¼Œè² è²¬éŒ„éŸ³ä¸¦é€²è¡ŒVADæª¢æ¸¬
    """
    def __init__(self, controller, sample_rate=16000, vad_mode=3):
        self.controller = controller
        self.sample_rate = sample_rate
        self.frame_duration_ms = 30  # VADå¹€é•·åº¦ï¼ˆæ¯«ç§’ï¼‰
        self.frame_size = int(sample_rate * self.frame_duration_ms / 1000)
        self.silence_threshold = 0.01  # éŸ³é‡é–¾å€¼
        self.silence_duration = 1.5  # é€£çºŒéœéŸ³åˆ¤å®šç‚ºèªå¥çµæŸçš„æ™‚é–“ï¼ˆç§’ï¼‰
        self.buffer_duration = 5  # æœ€å¤§ç·©è¡æ™‚é–“ï¼ˆç§’ï¼‰
        
        # åˆå§‹åŒ–VAD
        self.vad = webrtcvad.Vad()
        self.vad.set_mode(vad_mode)  # 0-3, 3ç‚ºæœ€åš´æ ¼
        
        # åˆå§‹åŒ–éŸ³è¨Šä½‡åˆ—å’Œç‹€æ…‹è®Šæ•¸
        self.audio_queue = queue.Queue()
        self.speech_queue = queue.Queue()
        self.running = False
        self.is_speaking = False
        self.last_speech_time = 0
        self.continuous_silence = 0
        self.energy_threshold = 0.0005  # èƒ½é‡é–¾å€¼ï¼Œç”¨æ–¼è¼”åŠ©VAD
        self.speech_frames = deque(maxlen=int(self.buffer_duration * 1000 / self.frame_duration_ms))
    
    def audio_callback(self, indata, frames, time, status):
        """éŸ³è¨Šå›èª¿å‡½æ•¸ï¼Œå°‡éŒ„è£½çš„éŸ³è¨Šæ”¾å…¥ä½‡åˆ—"""
        if status:
            print(f"éŒ¯èª¤: {status}")
        self.audio_queue.put(indata.copy())
    
    def is_speech(self, audio_frame):
        """èªéŸ³æ´»å‹•æª¢æ¸¬å‡½æ•¸"""
        # è½‰æ›ç‚ºPCM16æ ¼å¼ï¼ˆVADéœ€è¦ï¼‰
        pcm_data = (audio_frame * 32768).astype(np.int16).tobytes()
        
        # ä½¿ç”¨WebRTC VAD
        try:
            is_vad_speech = self.vad.is_speech(pcm_data, self.sample_rate)
        except:
            is_vad_speech = False
        
        # è¨ˆç®—éŸ³é »èƒ½é‡ä½œç‚ºè¼”åŠ©åˆ¤æ–·
        energy = np.mean(audio_frame**2)
        is_energy_speech = energy > self.energy_threshold
        
        # çµåˆVADå’Œèƒ½é‡åˆ¤æ–·
        return is_vad_speech or is_energy_speech
    
    def detect_sentence_boundary(self, current_silence):
        """æª¢æ¸¬èªå¥é‚Šç•Œ"""
        # å¦‚æœé€£çºŒéœéŸ³è¶…éé–¾å€¼ï¼Œåˆ¤å®šç‚ºèªå¥çµæŸ
        if current_silence >= self.silence_duration:
            return True
        
        # èªèª¿ä¸‹é™æª¢æ¸¬ï¼ˆç°¡åŒ–ç‰ˆï¼‰
        if len(self.speech_frames) >= 10:
            recent_energies = [np.mean(frame**2) for frame in list(self.speech_frames)[-10:]]
            if len(recent_energies) >= 5 and all(recent_energies[i] > recent_energies[i+1] for i in range(len(recent_energies)-5, len(recent_energies)-1)):
                return True
        
        return False
    
    def process_audio(self):
        """è™•ç†éŸ³è¨Šï¼Œé€²è¡ŒVADæª¢æ¸¬å’Œèªå¥åˆ‡å‰²"""
        audio_buffer = []
        buffer_duration = 0
        
        while self.running:
            # Check if we should be listening
            if not self.controller.should_listen():
                time.sleep(0.1)
                continue
                
            try:
                # Get audio data
                audio_data = self.audio_queue.get(timeout=0.1)
                
                # Process audio in frames for VAD
                for i in range(0, len(audio_data), self.frame_size):
                    if i + self.frame_size <= len(audio_data):
                        frame = audio_data[i:i+self.frame_size]
                        
                        # Check if this is speech
                        speech_detected = self.is_speech(frame)
                        self.speech_frames.append(frame)
                        
                        if speech_detected:
                            # Speech detected
                            if not self.is_speaking:
                                print("æª¢æ¸¬åˆ°èªéŸ³ï¼Œé–‹å§‹éŒ„è£½...")
                                self.is_speaking = True
                                
                                # If system is currently speaking, attempt to interrupt
                                if self.controller.can_be_interrupted():
                                    print("æª¢æ¸¬åˆ°ç”¨æˆ¶æƒ³æ‰“æ–·å°è©±...")
                                    self.controller.interrupt()
                            
                            # Update last speech time
                            self.last_speech_time = time.time()
                            self.continuous_silence = 0
                            
                            # Add to buffer
                            audio_buffer.append(frame)
                            buffer_duration += len(frame) / self.sample_rate
                        else:
                            # Process silence during speech
                            if self.is_speaking:
                                # Calculate silence duration
                                current_time = time.time()
                                self.continuous_silence = current_time - self.last_speech_time
                                
                                # Add silence frame to buffer (to preserve natural pauses)
                                audio_buffer.append(frame)
                                buffer_duration += len(frame) / self.sample_rate
                                
                                # Check for sentence boundary
                                if self.detect_sentence_boundary(self.continuous_silence):
                                    # End of sentence, submit audio for processing
                                    if buffer_duration > 0.5:  # At least 0.5s to process
                                        print("æª¢æ¸¬åˆ°èªå¥çµæŸï¼Œé€å‡ºè™•ç†...")
                                        
                                        # Combine audio data and send
                                        audio_chunk = np.concatenate(audio_buffer)
                                        self.speech_queue.put(audio_chunk.flatten())
                                    
                                    # Reset buffer and state
                                    audio_buffer = []
                                    buffer_duration = 0
                                    self.is_speaking = False
                        
                        # Avoid buffer overflow
                        if buffer_duration > self.buffer_duration:
                            print("ç·©è¡å€å·²æ»¿ï¼Œè™•ç†ç•¶å‰éŸ³è¨Š...")
                            audio_chunk = np.concatenate(audio_buffer)
                            
                            # Send audio for processing
                            self.speech_queue.put(audio_chunk.flatten())
                            
                            # Keep last 0.5s of audio to avoid cutting words
                            last_second_samples = int(self.sample_rate * 0.5)
                            if len(audio_chunk) > last_second_samples:
                                audio_buffer = [audio_chunk[-last_second_samples:]]
                                buffer_duration = 0.5
                            else:
                                audio_buffer = []
                                buffer_duration = 0
                
            except queue.Empty:
                # Queue is empty, check if we need to process remaining audio
                if self.is_speaking and time.time() - self.last_speech_time > self.silence_duration:
                    if audio_buffer and buffer_duration > 0.5:  # At least 0.5s to process
                        print("é•·æ™‚é–“éœéŸ³ï¼Œè™•ç†å‰©é¤˜èªéŸ³...")
                        
                        # Combine audio data and send
                        audio_chunk = np.concatenate(audio_buffer)
                        self.speech_queue.put(audio_chunk.flatten())
                        
                        # Reset buffer and state
                        audio_buffer = []
                        buffer_duration = 0
                        self.is_speaking = False
    
    def start(self):
        """å•Ÿå‹•éŸ³è¨Šæ“·å–"""
        self.running = True
        self.process_thread = threading.Thread(target=self.process_audio)
        self.process_thread.daemon = True
        self.process_thread.start()
        
        self.stream = sd.InputStream(
            callback=self.audio_callback,
            channels=1,
            samplerate=self.sample_rate,
            dtype='float32', 
            blocksize=self.frame_size
        )
        self.stream.start()
        print("éŸ³è¨Šæ“·å–æ¨¡çµ„å·²å•Ÿå‹•")
    
    def stop(self):
        """åœæ­¢éŸ³è¨Šæ“·å–"""
        self.running = False
        if hasattr(self, 'stream') and self.stream:
            self.stream.stop()
            self.stream.close()
        if hasattr(self, 'process_thread') and self.process_thread:
            self.process_thread.join(timeout=1)
        print("éŸ³è¨Šæ“·å–æ¨¡çµ„å·²åœæ­¢")


class SpeechRecognition:
    """
    èªéŸ³è¾¨è­˜æ¨¡çµ„ï¼Œä½¿ç”¨Whisperé€²è¡ŒèªéŸ³-æ–‡å­—è½‰æ›
    """
    def __init__(self, controller, speech_queue, text_queue, language="zh"):
        self.controller = controller
        self.speech_queue = speech_queue
        self.text_queue = text_queue
        self.language = language
        self.running = False
        
        # å»ºç«‹ç°¡è½‰ç¹è½‰æ›å™¨
        self.cc = OpenCC('s2t')
        
        # è¼‰å…¥Whisperæ¨¡å‹
        print("æ­£åœ¨è¼‰å…¥Whisperæ¨¡å‹ï¼Œè«‹ç¨å€™...")
        self.model = whisper.load_model("small")
        print("Whisperæ¨¡å‹è¼‰å…¥å®Œæˆ!")
    
    def process_speech(self):
        """è™•ç†èªéŸ³ä¸¦é€²è¡Œè­˜åˆ¥"""
        MIN_AUDIO_SAMPLES = 8000  # About 0.5s of audio

        while self.running:
            try:
                # Get audio data from queue
                audio_chunk = self.speech_queue.get(timeout=0.5)

                if len(audio_chunk) < MIN_AUDIO_SAMPLES:
                    print("éŸ³è¨Šç‰‡æ®µå¤ªçŸ­ï¼Œè·³éè­˜åˆ¥")
                    continue

                # Notify controller that we're thinking
                self.controller.start_thinking()
                print("æ­£åœ¨è­˜åˆ¥èªéŸ³...")

                # Use Whisper for transcription
                result = self.model.transcribe(
                    audio_chunk,
                    language=self.language,
                    task="transcribe"
                )

                # Convert simplified to traditional Chinese
                if result["text"].strip():
                    text = self.cc.convert(result["text"]).strip()
                    print(f"è­˜åˆ¥çµæœ: {text}")

                    # Put text result into queue
                    if text:
                        self.text_queue.put(text)

                        # Check for interrupt command
                        if self.controller.can_be_interrupted() and any(
                            keyword in text for keyword in ["åœæ­¢", "æ‰“æ–·", "æš«åœ", "ç­‰ä¸€ä¸‹"]
                        ):
                            print("æª¢æ¸¬åˆ°æ‰“æ–·æŒ‡ä»¤")
                            if self.controller.interrupt():
                                print("å·²æ‰“æ–·ç›®å‰å›æ‡‰")
                else:
                    print("æœªèƒ½è­˜åˆ¥æœ‰æ•ˆå…§å®¹")

                self.speech_queue.task_done()

            except queue.Empty:
                pass
            except Exception as e:
                print(f"èªéŸ³è­˜åˆ¥éŒ¯èª¤: {e}")

    
    def start(self):
        """å•Ÿå‹•èªéŸ³è¾¨è­˜"""
        self.running = True
        self.process_thread = threading.Thread(target=self.process_speech)
        self.process_thread.daemon = True
        self.process_thread.start()
        print("èªéŸ³è¾¨è­˜æ¨¡çµ„å·²å•Ÿå‹•")
    
    def stop(self):
        """åœæ­¢èªéŸ³è¾¨è­˜"""
        self.running = False
        if hasattr(self, 'process_thread') and self.process_thread:
            self.process_thread.join(timeout=1)
        print("èªéŸ³è¾¨è­˜æ¨¡çµ„å·²åœæ­¢")


class LLMConversation:
    """
    LLMå°è©±æ¨¡çµ„ï¼Œä½¿ç”¨Ollamaé€²è¡Œæ–‡å­—å°è©±
    """
    def __init__(self, controller, text_queue, response_queue, 
                 model="gemma3:12b", api_url="http://localhost:11434/api",
                 system_prompt=None, max_memory=10):
        self.controller = controller
        self.text_queue = text_queue
        self.response_queue = response_queue
        self.model = model
        self.api_url = api_url
        self.system_prompt = system_prompt or """ä½ æ˜¯ä¸€å€‹å‹å–„ã€æœ‰å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚"""
        self.max_memory = max_memory
        self.conversation_history = []
        self.running = False
        
        # åˆå§‹åŒ–å°è©±æ­·å²
        if self.system_prompt:
            self.conversation_history.append({"role": "system", "content": self.system_prompt})
    
    def add_to_history(self, role, content):
        """æ·»åŠ å°è©±å…§å®¹åˆ°æ­·å²è¨˜éŒ„"""
        # ç¢ºä¿ system prompt åªåœ¨æ­·å²ä¸­å‡ºç¾ä¸€æ¬¡
        if role == "system":
            for i, msg in enumerate(self.conversation_history):
                if msg["role"] == "system":
                    self.conversation_history[i]["content"] = content
                    return
        
        self.conversation_history.append({"role": role, "content": content})
        
        # å¦‚æœè¶…å‡ºæœ€å¤§è¨˜æ†¶å®¹é‡ï¼Œç§»é™¤æœ€æ—©çš„å°è©±
        if len(self.conversation_history) > self.max_memory * 2 + 1:  # +1æ˜¯å› ç‚ºsystem prompt
            # ä¿ç•™system prompt
            system_prompt = self.conversation_history[0] if self.conversation_history[0]["role"] == "system" else None
            self.conversation_history = self.conversation_history[-(self.max_memory*2):]
            if system_prompt and self.conversation_history[0]["role"] != "system":
                self.conversation_history.insert(0, system_prompt)
    
    def clear_history(self):
        """æ¸…é™¤å°è©±æ­·å²ï¼Œä½†ä¿ç•™system prompt"""
        if self.conversation_history and self.conversation_history[0]["role"] == "system":
            system_prompt = self.conversation_history[0]
            self.conversation_history = [system_prompt]
        else:
            self.conversation_history = []
            if self.system_prompt:
                self.conversation_history.append({"role": "system", "content": self.system_prompt})
        print("å°è©±æ­·å²å·²æ¸…é™¤")
    
    def query_ollama(self, prompt):
        """å‘Ollamaç™¼é€è«‹æ±‚ä¸¦ç²å–å›æ‡‰"""
        url = f"{self.api_url}/chat"
        
        # æ·»åŠ ç•¶å‰æå•åˆ°æ­·å²è¨˜éŒ„
        self.add_to_history("user", prompt)
        
        payload = {
            "model": self.model,
            "messages": self.conversation_history,
            "stream": False
        }
        
        try:
            response = requests.post(url, json=payload)
            
            if response.status_code == 200:
                result = response.json()
                ai_message = result.get('message', {}).get('content', '')
                
                # æ·»åŠ AIå›æ‡‰åˆ°æ­·å²è¨˜éŒ„
                if ai_message:
                    self.add_to_history("assistant", ai_message)
                
                return ai_message
            else:
                print(f"Ollama APIéŒ¯èª¤: {response.status_code}")
                print(response.text)
                return "æŠ±æ­‰ï¼Œæˆ‘è™•ç†æ‚¨çš„è«‹æ±‚æ™‚é‡åˆ°äº†å•é¡Œã€‚"
        except Exception as e:
            print(f"èª¿ç”¨Ollama APIæ™‚å‡ºéŒ¯: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•é€£æ¥åˆ°èªè¨€æ¨¡å‹æœå‹™ã€‚"
    
    def process_messages(self):
        """è™•ç†è¼¸å…¥æ–‡å­—ä¸¦ç”Ÿæˆå›æ‡‰"""
        while self.running:
            try:
                # å¾ä½‡åˆ—ç²å–æ–‡å­—
                text = self.text_queue.get(timeout=0.5)
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºå‘½ä»¤
                if text.lower() in ['æ¸…é™¤æ­·å²', 'clear history']:
                    self.clear_history()
                    self.response_queue.put("å°è©±æ­·å²å·²æ¸…é™¤ã€‚")
                    continue
                
                # é€šçŸ¥æ§åˆ¶å™¨æ­£åœ¨æ€è€ƒ
                self.controller.start_thinking()
                print(f"ğŸ¤– è™•ç†ç”¨æˆ¶è¼¸å…¥: {text}")
                
                # æŸ¥è©¢LLMä¸¦ç²å–å›æ‡‰
                response = self.query_ollama(text)
                print(f"LLMå›æ‡‰: {response}")
                
                # å°‡å›æ‡‰æ”¾å…¥ä½‡åˆ—
                if response:
                    self.response_queue.put(response)
                
                self.text_queue.task_done()
                
            except queue.Empty:
                pass
            except Exception as e:
                print(f"LLMè™•ç†éŒ¯èª¤: {e}")
                self.response_queue.put("è™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚")
    
    def start(self):
        """å•Ÿå‹•LLMå°è©±"""
        self.running = True
        self.process_thread = threading.Thread(target=self.process_messages)
        self.process_thread.daemon = True
        self.process_thread.start()
        print("LLMå°è©±æ¨¡çµ„å·²å•Ÿå‹•")
    
    def stop(self):
        """åœæ­¢LLMå°è©±"""
        self.running = False
        if hasattr(self, 'process_thread') and self.process_thread:
            self.process_thread.join(timeout=1)
        print("LLMå°è©±æ¨¡çµ„å·²åœæ­¢")
    
    def save_conversation(self, filename="conversation_history.json"):
        """å°‡å°è©±æ­·å²ä¿å­˜åˆ°æª”æ¡ˆ"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
            print(f"å°è©±æ­·å²å·²ä¿å­˜åˆ° {filename}")
            return True
        except Exception as e:
            print(f"ä¿å­˜å°è©±æ­·å²æ™‚å‡ºéŒ¯: {e}")
            return False
    
    def load_conversation(self, filename="conversation_history.json"):
        """å¾æª”æ¡ˆè¼‰å…¥å°è©±æ­·å²"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                loaded_history = json.load(f)
                
                # æª¢æŸ¥æ˜¯å¦æœ‰system prompt
                has_system = any(msg.get("role") == "system" for msg in loaded_history)
                
                if not has_system and self.system_prompt:
                    loaded_history.insert(0, {"role": "system", "content": self.system_prompt})
                
                self.conversation_history = loaded_history
                
            print(f"å·²å¾ {filename} è¼‰å…¥å°è©±æ­·å²")
            return True
        except FileNotFoundError:
            print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ {filename}")
            return False
        except Exception as e:
            print(f"è¼‰å…¥å°è©±æ­·å²æ™‚å‡ºéŒ¯: {e}")
            return False


class VoiceSynthesis:
    """
    èªéŸ³åˆæˆæ¨¡çµ„ï¼Œä½¿ç”¨GPT-SoVITSé€²è¡Œæ–‡å­—-èªéŸ³è½‰æ›
    """
    def __init__(self, controller, response_queue, 
                 gpt_sovits_url="http://localhost:9880",
                 ref_audio_path=None, 
                 gpt_weights_path=None, 
                 sovits_weights_path=None):
        self.controller = controller
        self.response_queue = response_queue
        self.gpt_sovits_url = gpt_sovits_url
        self.ref_audio_path = ref_audio_path
        self.audio_queue = queue.Queue()
        self.running = False
        self.stop_playing = False
        
        # å¦‚æœæä¾›äº†æ¨¡å‹æ¬Šé‡è·¯å¾‘ï¼Œè¨­ç½®æ¨¡å‹æ¬Šé‡
        if gpt_weights_path:
            self.set_gpt_weights(gpt_weights_path)
        if sovits_weights_path:
            self.set_sovits_weights(sovits_weights_path)
    
    def set_gpt_weights(self, weights_path):
        """è¨­ç½®GPTæ¨¡å‹æ¬Šé‡"""
        url = f"{self.gpt_sovits_url}/set_gpt_weights"
        params = {"weights_path": weights_path}
        try:
            response = requests.get(url, params=params)
            if response.status_code == 200:
                print("GPTæ¬Šé‡è¨­ç½®æˆåŠŸ")
            else:
                print(f"GPTæ¬Šé‡è¨­ç½®å¤±æ•—: {response.text}")
        except Exception as e:
            print(f"è¨­ç½®GPTæ¬Šé‡æ™‚å‡ºéŒ¯: {e}")
    
    def set_sovits_weights(self, weights_path):
        """è¨­ç½®SoVITSæ¨¡å‹æ¬Šé‡"""
        url = f"{self.gpt_sovits_url}/set_sovits_weights"
        params = {"weights_path": weights_path}
        try:
            response = requests.get(url, params=params)
            if response.status_code == 200:
                print("SoVITSæ¬Šé‡è¨­ç½®æˆåŠŸ")
            else:
                print(f"SoVITSæ¬Šé‡è¨­ç½®å¤±æ•—: {response.text}")
        except Exception as e:
            print(f"è¨­ç½®SoVITSæ¬Šé‡æ™‚å‡ºéŒ¯: {e}")
    
    def set_ref_audio(self, ref_audio_path):
        """è¨­ç½®åƒè€ƒéŸ³æª”è·¯å¾‘"""
        self.ref_audio_path = ref_audio_path
        print(f"åƒè€ƒéŸ³æª”å·²è¨­ç½®ç‚º: {ref_audio_path}")
    
    def text_to_speech(self, text, text_lang="zh", prompt_text=None):
        """ä½¿ç”¨GPT-SoVITSå°‡æ–‡æœ¬è½‰æ›ç‚ºèªéŸ³"""
        url = f"{self.gpt_sovits_url}/tts"
        
        if not self.ref_audio_path:
            print("æœªè¨­ç½®åƒè€ƒéŸ³æª”è·¯å¾‘!")
            return None
        
        payload = {
            "text": text,
            "text_lang": text_lang,
            "ref_audio_path": self.ref_audio_path,
            "prompt_lang": text_lang,
            "prompt_text": prompt_text or "èŒ¶ä¼šæ˜¯æ·‘å¥³çš„å¿…ä¿®è¯¾ï¼Œå¦‚æœä½ æƒ³å­¦ä¹ èŒ¶ä¼šç¤¼ä»ªçš„è¯ï¼Œæˆ‘å¯ä»¥æ•™ä½ å“¦",
            "speed_factor": 1.0,
            "media_type": "wav"
        }
        
        try:
            response = requests.post(url, json=payload)
            
            if response.status_code == 200:
                # è¿”å›çš„æ˜¯éŸ³æª”æ•¸æ“š
                audio_data = response.content
                return audio_data
            else:
                print(f"GPT-SoVITS APIéŒ¯èª¤: {response.status_code}")
                print(response.text)
                return None
        except Exception as e:
            print(f"èª¿ç”¨GPT-SoVITSæ™‚å‡ºéŒ¯: {e}")
            return None
    
    def play_audio(self, audio_data):
        """æ’­æ”¾éŸ³æª”æ•¸æ“š"""
        try:
            # å°‡äºŒé€²åˆ¶éŸ³æª”æ•¸æ“šè½‰æ›ç‚ºNumPyæ•¸çµ„
            audio_np, sample_rate = sf.read(io.BytesIO(audio_data))
            
            # æ’­æ”¾éŸ³æª”
            sd.play(audio_np, sample_rate)
            sd.wait()  # ç­‰å¾…éŸ³æª”æ’­æ”¾å®Œæˆ
        except Exception as e: 
            print(f"æ’­æ”¾éŸ³æª”æ™‚å‡ºéŒ¯: {e}")
    
    def audio_player_thread(self):
        """éŸ³æª”æ’­æ”¾ç·šç¨‹"""
        while not self.stop_playing:
            try:
                # Check if interrupted before getting next audio
                if self.controller.interrupted:
                    print("æª¢æ¸¬åˆ°æ‰“æ–·ï¼Œæ¸…ç©ºéŸ³è¨Šä½‡åˆ—")
                    # Empty the queue
                    while not self.audio_queue.empty():
                        try:
                            self.audio_queue.get_nowait()
                            self.audio_queue.task_done()
                        except queue.Empty:
                            break
                    # Reset interrupted flag after handling
                    self.controller.interrupted = False
                    self.controller.stop_speaking()
                    continue
                    
                # Get audio data with timeout
                audio_data = self.audio_queue.get(timeout=1)
                
                # Check interrupt flag again before playing
                if self.controller.interrupted:
                    print("æª¢æ¸¬åˆ°æ‰“æ–·ï¼Œåœæ­¢ç•¶å‰éŸ³è¨Šæ’­æ”¾")
                    sd.stop()  # Stop current playback
                    self.audio_queue.task_done()
                    continue
                
                # Play audio
                self.controller.start_speaking()
                self.play_audio(audio_data)
                self.audio_queue.task_done()
                
                # Check if queue is empty
                if self.audio_queue.empty():
                    self.controller.stop_speaking()
                
            except queue.Empty:
                pass  # Queue is empty, continue waiting
            except Exception as e:
                print(f"éŸ³æª”æ’­æ”¾ç·šç¨‹éŒ¯èª¤: {e}")
                self.controller.stop_speaking()
    
    def process_responses(self):
        """è™•ç†æ–‡å­—å›æ‡‰ä¸¦è½‰æ›ç‚ºèªéŸ³"""
        while self.running:
            try:
                # Get text response from queue
                response = self.response_queue.get(timeout=0.5)
                if not response:
                    continue
                
                print("æ­£åœ¨ç”ŸæˆèªéŸ³...")
                
                # Split into sentences based on punctuation
                sentences = []
                current_sentence = ""
                
                for char in response:
                    current_sentence += char
                    if char in ["ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]:
                        if current_sentence.strip():
                            sentences.append(current_sentence.strip())
                        current_sentence = ""
                
                # Add the last sentence if any
                if current_sentence.strip():
                    sentences.append(current_sentence.strip())
                
                # Process long sentences
                processed_sentences = []
                for sentence in sentences:
                    # Split sentences longer than 50 characters at commas
                    if len(sentence) > 50:
                        parts = []
                        current_part = ""
                        for char in sentence:
                            current_part += char
                            if (char in ["ï¼Œ", "ï¼›", ",", ";"]) and len(current_part) > 20:
                                parts.append(current_part)
                                current_part = ""
                        if current_part:
                            parts.append(current_part)
                        processed_sentences.extend(parts)
                    else:
                        processed_sentences.append(sentence)
                
                # Generate and add to playback queue
                for sentence in processed_sentences:
                    # Check for interruption before generating each sentence
                    if self.controller.interrupted:
                        print("æª¢æ¸¬åˆ°æ‰“æ–·ï¼Œåœæ­¢ç”Ÿæˆæ›´å¤šèªéŸ³")
                        break
                        
                    if sentence:  # Ensure sentence is not empty
                        audio_data = self.text_to_speech(sentence)
                        if audio_data:
                            self.audio_queue.put(audio_data)
                
                self.response_queue.task_done()
                
            except queue.Empty:
                pass
            except Exception as e:
                print(f"èªéŸ³åˆæˆéŒ¯èª¤: {e}")
    
    def start(self):
        """å•Ÿå‹•èªéŸ³åˆæˆ"""
        self.running = True
        self.stop_playing = False
        
        # å•Ÿå‹•è™•ç†å›æ‡‰çš„ç·šç¨‹
        self.process_thread = threading.Thread(target=self.process_responses)
        self.process_thread.daemon = True
        self.process_thread.start()
        
        # å•Ÿå‹•éŸ³é »æ’­æ”¾ç·šç¨‹
        self.player_thread = threading.Thread(target=self.audio_player_thread)
        self.player_thread.daemon = True
        self.player_thread.start()
        
        print("ğŸ”Š èªéŸ³åˆæˆæ¨¡çµ„å·²å•Ÿå‹•")
    
    def stop(self):
        """åœæ­¢èªéŸ³åˆæˆ"""
        self.running = False
        self.stop_playing = True
        
        # åœæ­¢ç•¶å‰æ’­æ”¾
        sd.stop()
        
        # æ¸…ç©ºä½‡åˆ—
        while not self.audio_queue.empty():
            try:
                self.audio_queue.get_nowait()
                self.audio_queue.task_done()
            except queue.Empty:
                break
        
        if hasattr(self, 'process_thread') and self.process_thread:
            self.process_thread.join(timeout=1)
        if hasattr(self, 'player_thread') and self.player_thread:
            self.player_thread.join(timeout=1)
        
        print("èªéŸ³åˆæˆæ¨¡çµ„å·²åœæ­¢")
        
class VoiceLLMChatSystem:
    """
    ä¸»æ•´åˆç³»çµ±é¡åˆ¥ï¼Œè² è²¬æ•´åˆæ‰€æœ‰æ¨¡çµ„ä¸¦æä¾›ä½¿ç”¨è€…ä»‹é¢
    """
    def __init__(self, 
                ollama_model="gemma3:12b", 
                ollama_api_url="http://localhost:11434/api",
                gpt_sovits_url="http://localhost:9880",
                ref_audio_path=None,
                gpt_weights_path=None,
                sovits_weights_path=None,
                system_prompt=None,
                max_memory=10):
        """
        åˆå§‹åŒ–æ•´åˆç³»çµ±
        
        åƒæ•¸:
        ollama_model: è¦ä½¿ç”¨çš„Ollamaæ¨¡å‹åç¨±
        ollama_api_url: Ollama APIçš„åŸºç¤URL
        gpt_sovits_url: GPT-SoVITS APIçš„URL
        ref_audio_path: åƒè€ƒéŸ³æª”è·¯å¾‘ï¼Œç”¨æ–¼ç¢ºå®šèªéŸ³é¢¨æ ¼
        gpt_weights_path: GPTæ¨¡å‹æ¬Šé‡è·¯å¾‘
        sovits_weights_path: SoVITSæ¨¡å‹æ¬Šé‡è·¯å¾‘
        system_prompt: è¨­å®šLLMçš„system prompt
        max_memory: è¨˜æ†¶çš„æœ€å¤§å°è©±å›åˆæ•¸
        """
        # åˆå§‹åŒ–å°è©±æ§åˆ¶å™¨
        self.controller = DialogController()
        
        # åˆå§‹åŒ–ä½‡åˆ—
        self.speech_queue = queue.Queue()  # éŸ³è¨Šç‰‡æ®µä½‡åˆ—
        self.text_queue = queue.Queue()    # è­˜åˆ¥æ–‡å­—ä½‡åˆ—
        self.response_queue = queue.Queue() # å›æ‡‰æ–‡å­—ä½‡åˆ—
        
        # åˆå§‹åŒ–éŸ³è¨Šæ“·å–æ¨¡çµ„
        self.audio_capture = AudioCapture(self.controller)
        
        # åˆå§‹åŒ–èªéŸ³è¾¨è­˜æ¨¡çµ„
        self.speech_recognition = SpeechRecognition(
            self.controller, 
            self.audio_capture.speech_queue,
            self.text_queue
        )
        
        # åˆå§‹åŒ–LLMå°è©±æ¨¡çµ„
        self.llm_conversation = LLMConversation(
            self.controller,
            self.text_queue,
            self.response_queue,
            model=ollama_model,
            api_url=ollama_api_url,
            system_prompt=system_prompt,
            max_memory=max_memory
        )
        
        # åˆå§‹åŒ–èªéŸ³åˆæˆæ¨¡çµ„
        self.voice_synthesis = VoiceSynthesis(
            self.controller,
            self.response_queue,
            gpt_sovits_url=gpt_sovits_url,
            ref_audio_path=ref_audio_path,
            gpt_weights_path=gpt_weights_path,
            sovits_weights_path=sovits_weights_path
        )
        
        # ç‹€æ…‹è®Šæ•¸
        self.running = False
        self.status_thread = None
    
    def start(self):
        """å•Ÿå‹•æ‰€æœ‰ç³»çµ±æ¨¡çµ„"""
        print("=== èªéŸ³å°è©±ç³»çµ±å•Ÿå‹•ä¸­ ===")
        
        # å•Ÿå‹•å„æ¨¡çµ„
        self.audio_capture.start()
        self.speech_recognition.start()
        self.llm_conversation.start()
        self.voice_synthesis.start()
        
        # è¨­ç½®ç‹€æ…‹ç›£æ§
        self.running = True
        self.status_thread = threading.Thread(target=self._status_monitor)
        self.status_thread.daemon = True
        self.status_thread.start()
        
        print("ç³»çµ±å·²å•Ÿå‹•ï¼Œè«‹é–‹å§‹å°è©±...")
    
    def stop(self):
        """åœæ­¢æ‰€æœ‰ç³»çµ±æ¨¡çµ„"""
        print("æ­£åœ¨é—œé–‰ç³»çµ±...")
        self.running = False
        
        # åœæ­¢å„æ¨¡çµ„
        self.voice_synthesis.stop()
        self.llm_conversation.stop()
        self.speech_recognition.stop()
        self.audio_capture.stop()
        
        if self.status_thread:
            self.status_thread.join(timeout=1)
        
        print("ç³»çµ±å·²é—œé–‰")
    
    def _status_monitor(self):
        """ç‹€æ…‹ç›£æ§ç·šç¨‹"""
        last_state = ""
        while self.running:
            current_state = self.controller.get_state()
            
            # ç‹€æ…‹è®ŠåŒ–æ™‚é¡¯ç¤º
            if current_state != last_state:
                status_map = {
                    "idle": "ğŸ›Œ ç©ºé–’ä¸­",
                    "listening": "è†è½ä¸­...",
                    "thinking": "æ€è€ƒä¸­...",
                    "speaking": "èªªè©±ä¸­..."
                }
                print(status_map.get(current_state, current_state))
                last_state = current_state
            
            time.sleep(0.5)
    
    def save_conversation(self, filename="conversation_history.json"):
        """ä¿å­˜å°è©±æ­·å²"""
        return self.llm_conversation.save_conversation(filename)
    
    def load_conversation(self, filename="conversation_history.json"):
        """è¼‰å…¥å°è©±æ­·å²"""
        return self.llm_conversation.load_conversation(filename)
    
    def clear_history(self):
        """æ¸…é™¤å°è©±æ­·å²"""
        return self.llm_conversation.clear_history()
    
    def set_ref_audio(self, ref_audio_path):
        """è¨­ç½®åƒè€ƒéŸ³æª”"""
        self.voice_synthesis.set_ref_audio(ref_audio_path)
    
    def run_console_ui(self):
        """åŸ·è¡Œä¸€å€‹ç°¡å–®çš„æ§åˆ¶å°UI"""
        try:
            self.start()
            print("\n=== èªéŸ³å°è©±ç³»çµ±å·²å•Ÿå‹• ===")
            print("è«‹å°è‘—éº¥å…‹é¢¨èªªè©±...")
            print("æ§åˆ¶æŒ‡ä»¤ï¼š")
            print("- ã€Œé€€å‡ºã€æˆ–ã€ŒçµæŸã€: é€€å‡ºç¨‹å¼")
            print("- ã€Œæ¸…é™¤æ­·å²ã€: æ¸…é™¤å°è©±è¨˜æ†¶")
            print("- ã€Œä¿å­˜å°è©±ã€: å°‡å°è©±æ­·å²ä¿å­˜åˆ°æª”æ¡ˆ")
            print("- ã€Œè¼‰å…¥å°è©±ã€: å¾æª”æ¡ˆè¼‰å…¥å°è©±æ­·å²")
            
            while True:
                cmd = input("\nè¼¸å…¥å‘½ä»¤ (æˆ–æŒ‰Enterç¹¼çºŒå°è©±): ")
                
                if cmd.lower() in ['é€€å‡º', 'çµæŸ', 'exit', 'quit']:
                    break
                elif cmd.lower() in ['æ¸…é™¤æ­·å²', 'clear history']:
                    self.clear_history()
                    print("å°è©±æ­·å²å·²æ¸…é™¤")
                elif cmd.lower() in ['ä¿å­˜å°è©±', 'save conversation']:
                    filename = input("è«‹è¼¸å…¥æª”æ¡ˆåç¨± (é è¨­ç‚º conversation_history.json): ").strip()
                    if not filename:
                        filename = "conversation_history.json"
                    if self.save_conversation(filename):
                        print(f"å°è©±æ­·å²å·²ä¿å­˜åˆ° {filename}")
                elif cmd.lower() in ['è¼‰å…¥å°è©±', 'load conversation']:
                    filename = input("è«‹è¼¸å…¥æª”æ¡ˆåç¨± (é è¨­ç‚º conversation_history.json): ").strip()
                    if not filename:
                        filename = "conversation_history.json"
                    if self.load_conversation(filename):
                        print(f"å·²å¾ {filename} è¼‰å…¥å°è©±æ­·å²")
                else:
                    print("ç¹¼çºŒå°è©±ä¸­...")
        
        except KeyboardInterrupt:
            print("\næª¢æ¸¬åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨é—œé–‰ç³»çµ±...")
        finally:
            self.stop()


# ç¨‹å¼é€²å…¥é»
if __name__ == "__main__":
    # è¨­å®šç³»çµ±åƒæ•¸
    OLLAMA_MODEL = "gemma3:12b"  # ä½¿ç”¨çš„Ollamaæ¨¡å‹
    OLLAMA_API_URL = "http://localhost:11434/api"  # Ollama API URL
    GPT_SOVITS_URL = "http://localhost:9880"  # GPT-SoVITS API URL
    
    # åƒè€ƒéŸ³æª”è·¯å¾‘ï¼ˆæ›¿æ›ç‚ºå¯¦éš›è·¯å¾‘ï¼‰
    REF_AUDIO_PATH = "C:/GPT-SoVITS/èŠ™å®å¨œ/å‚è€ƒéŸ³é¢‘/ã€æ­£å¸¸ã€‘èŒ¶ä¼šæ˜¯æ·‘å¥³çš„å¿…ä¿®è¯¾ï¼Œå¦‚æœä½ æƒ³å­¦ä¹ èŒ¶ä¼šç¤¼ä»ªçš„è¯ï¼Œæˆ‘å¯ä»¥æ•™ä½ å“¦.wav"
    
    # å¯é¸ï¼šGPTå’ŒSoVITSæ¨¡å‹æ¬Šé‡è·¯å¾‘
    GPT_WEIGHTS_PATH = "C:/GPT-SoVITS/èŠ™å®å¨œ/GPT_weights/Furina-e15.ckpt"  
    SOVITS_WEIGHTS_PATH = "C:/GPT-SoVITS/èŠ™å®å¨œ/SoVITS_weights/Furina_e8_s304.pth"
    
    # ç³»çµ±æç¤ºè©
    SYSTEM_PROMPT = """
    ä½ æ˜¯ä¸€å€‹å‹å–„ã€æœ‰å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚ä½ å…·æœ‰ä»¥ä¸‹ç‰¹é»ï¼š
    ä½ æ˜¯èŠ™å¯§å¨œï¼ŒæŸåœ‹çš„çµ±æ²»è€…èˆ‡çŸ¥åæ­ŒåŠ‡æ˜æ˜Ÿã€‚ä½ å€‹æ€§å¹½é»˜ï¼Œæ¨™æº–é«˜ï¼Œç§ä¸‹å–œæ„›å°å‹•ç‰©ã€‚
    """
    
    # å°è©±è¨˜æ†¶å®¹é‡ï¼ˆå›åˆæ•¸ï¼‰
    MAX_MEMORY = 10
    
    # å‰µå»ºä¸¦å•Ÿå‹•ç³»çµ±
    system = VoiceLLMChatSystem(
        ollama_model=OLLAMA_MODEL,
        ollama_api_url=OLLAMA_API_URL,
        gpt_sovits_url=GPT_SOVITS_URL,
        ref_audio_path=REF_AUDIO_PATH,
        gpt_weights_path=GPT_WEIGHTS_PATH,
        sovits_weights_path=SOVITS_WEIGHTS_PATH,
        system_prompt=SYSTEM_PROMPT,
        max_memory=MAX_MEMORY
    )
    
    # åŸ·è¡Œç³»çµ±
    system.run_console_ui()